{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e090df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sandralm\\AppData\\Local\\miniconda3\\envs\\IDS\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import kaggle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle # to save training history\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59ab23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './human-face-emotions/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc8df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57756 files belonging to 5 classes.\n",
      "Classes: ['Angry', 'Fear', 'Happy', 'Sad', 'Suprise']\n"
     ]
    }
   ],
   "source": [
    "full_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    label_mode=\"int\",\n",
    "    image_size=(48, 48),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=None,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "class_names = full_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a06b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = list(full_ds.as_numpy_iterator())\n",
    "\n",
    "images = [x[0] for x in full_data]  # list of arrays\n",
    "labels = [x[1] for x in full_data]  # list of ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    images, labels,\n",
    "    test_size=0.30,         # 30% val+test\n",
    "    random_state=42,\n",
    "    stratify=labels \n",
    ")\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp,\n",
    "    test_size=0.50,         # 15% val, 15% test\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ba4b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d455715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no doing this for transformer\n",
    "#batch_size = 128\n",
    "\n",
    "#train_ds = train_ds.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "#val_ds   = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "#test_ds  = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT required size is 224x224\n",
    "\n",
    "def preprocess_vit(image, label):\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "\n",
    "    # normalize pixel values\n",
    "    image = image / 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## originally created for vit but same works for ef\n",
    "\n",
    "vit_train_ds = train_ds.map(preprocess_vit).shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "vit_val_ds   = val_ds.map(preprocess_vit).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "vit_test_ds  = test_ds.map(preprocess_vit).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "## prefetch(tf.data.AUTOTUNE) - makes preloading dynamic --> faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3506257",
   "metadata": {},
   "source": [
    "### self trained VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f77bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size=4, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.projection = tf.keras.layers.Conv2D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"valid\"\n",
    "        )\n",
    "        self.flatten = tf.keras.layers.Reshape((-1, embed_dim))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len=1024, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=max_len,\n",
    "            output_dim=embed_dim\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]             \n",
    "        positions = tf.range(seq_len)         \n",
    "        pos_embed = self.pos_embedding(positions)\n",
    "        return x + pos_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1af22c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(mlp_dim, activation=\"gelu\"),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Dense(embed_dim),\n",
    "            tf.keras.layers.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.attn(x, x)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.mlp(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d71a5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_small_vit(input_shape=(48, 48, 1), num_classes=5):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # grayscale → RGB\n",
    "    x = tf.keras.layers.Lambda(lambda t: tf.concat([t, t, t], axis=-1))(inputs)\n",
    "\n",
    "    # resize to something divisible by 4\n",
    "    x = tf.keras.layers.Resizing(64, 64)(x)\n",
    "\n",
    "    # patches\n",
    "    x = PatchEmbedding(patch_size=4, embed_dim=64)(x)\n",
    "\n",
    "    # positional embeddings\n",
    "    x = PositionalEmbedding(max_len=1024, embed_dim=64)(x)\n",
    "\n",
    "    # transformer blocks\n",
    "    for _ in range(3):\n",
    "        x = TransformerEncoder(\n",
    "            embed_dim=64,\n",
    "            num_heads=2,\n",
    "            mlp_dim=128,\n",
    "            dropout=0.1\n",
    "        )(x)\n",
    "\n",
    "    # classifier head\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"gelu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9b5a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_small_vit()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(3e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a63f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=4,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e14d13a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 132ms/step - accuracy: 0.2967 - loss: 1.5777 - val_accuracy: 0.3157 - val_loss: 1.5497\n",
      "Epoch 2/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 224ms/step - accuracy: 0.3097 - loss: 1.5539 - val_accuracy: 0.3188 - val_loss: 1.5384\n",
      "Epoch 3/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 297ms/step - accuracy: 0.3129 - loss: 1.5467 - val_accuracy: 0.3166 - val_loss: 1.5597\n",
      "Epoch 4/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 335ms/step - accuracy: 0.3180 - loss: 1.5433 - val_accuracy: 0.3207 - val_loss: 1.5471\n",
      "Epoch 5/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 223ms/step - accuracy: 0.3141 - loss: 1.5473 - val_accuracy: 0.3218 - val_loss: 1.5402\n",
      "Epoch 6/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 190ms/step - accuracy: 0.3164 - loss: 1.5440 - val_accuracy: 0.3217 - val_loss: 1.5385\n"
     ]
    }
   ],
   "source": [
    "history_1 = model.fit(\n",
    "    train_ds.batch(32),\n",
    "    validation_data=val_ds.batch(32),\n",
    "    epochs=20,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "71002789",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv2d_4\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (224, 224, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Test Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_acc)\n",
      "File \u001b[1;32mc:\\Users\\sandralm\\AppData\\Local\\miniconda3\\envs\\IDS\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\sandralm\\AppData\\Local\\miniconda3\\envs\\IDS\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:202\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmin_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"conv2d_4\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (224, 224, 1)"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(\"Final Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257101b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sandralm\\AppData\\Local\\miniconda3\\envs\\IDS\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:107: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  return saving_lib.save_model(model, filepath)\n"
     ]
    }
   ],
   "source": [
    "model.save('models/small_ViT.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c13b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_history/history_small_ViT.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history_1.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0450121",
   "metadata": {},
   "source": [
    "### EfficientNetB0 - lightweight transformer-like CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c55ef3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (96, 96)\n",
    "\n",
    "def preprocess_ef(image, label):\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    image = image / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_ef = train_ds.map(preprocess_ef).shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ef   = val_ds.map(preprocess_ef).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ef  = test_ds.map(preprocess_ef).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20052261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resizing_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Resizing</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resizing_9 (\u001b[38;5;33mResizing\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "base = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(96, 96, 3),\n",
    "    pooling=\"avg\"\n",
    ")\n",
    "\n",
    "base.trainable = False\n",
    "\n",
    "model_ef = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(96, 96),\n",
    "    tf.keras.layers.Conv2D(3, (1,1)),  # grayscale --> RGB\n",
    "\n",
    "    base,\n",
    "\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_ef.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "433bdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ef.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f8487c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=4,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f303b914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 258ms/step - accuracy: 0.3037 - loss: 1.5790 - val_accuracy: 0.3113 - val_loss: 1.5687\n",
      "Epoch 2/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 375ms/step - accuracy: 0.3088 - loss: 1.5718 - val_accuracy: 0.3113 - val_loss: 1.5675\n",
      "Epoch 3/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 526ms/step - accuracy: 0.3133 - loss: 1.5689 - val_accuracy: 0.3113 - val_loss: 1.5676\n",
      "Epoch 4/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10710s\u001b[0m 8s/step - accuracy: 0.3079 - loss: 1.5716 - val_accuracy: 0.3113 - val_loss: 1.5695\n",
      "Epoch 5/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 570ms/step - accuracy: 0.3113 - loss: 1.5704 - val_accuracy: 0.3113 - val_loss: 1.5688\n",
      "Epoch 6/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4621s\u001b[0m 4s/step - accuracy: 0.3072 - loss: 1.5719 - val_accuracy: 0.3113 - val_loss: 1.5688\n"
     ]
    }
   ],
   "source": [
    "history_ef = model_ef.fit(\n",
    "    train_ef,\n",
    "    validation_data=val_ef,\n",
    "    epochs=30,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
